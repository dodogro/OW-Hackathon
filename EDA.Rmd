---
title: "EDA"
output: html_notebook
---

```{r Packages Loading}
pacman::p_load(ggplot2, tidyverse, lubridate, packcircles, viridis, ggiraph,
               lattice, gcookbook, tidyr, timechange, ggstream, dplyr)

options(scipen=999)
```

```{r Loading Dataset}
path = "../Data-OW/"
load(file = paste0(path,"MergedData.RData"))
df <- transaction.df
remove(transaction.df)
```

```{r Summary 2020}
summary(df)
```

# Check for drastic data changes
We expect the same order of magnitude in the number of transaction recorded for each month during the three years. If that is not the case, probably there has been some mistakes in the data collection process.

```{r Transaction Quantities}
trans.number <- df %>% 
  group_by(Month = month(TransactionDate), Year = year(TransactionDate)) %>% 
  summarise(trans.record = n()) %>% 
  arrange(Year)

ggplot(data = trans.number, aes(x = Month, y = trans.record, group = Year,
                                colour = factor(Year))) +
  geom_line() +
  geom_point() + 
  scale_x_discrete(limits = trans.number$Month) +
  ylab("Number of transactions")

```
Huge decrease in April 2020 with 151128 transactions considered a mean equal to 313250.6 monthly transactions

Going deep into April 2020
```{r April deep dive}
april2020 <-  df %>% 
  filter(year(TransactionDate) == 2020) %>% 
  filter(month(TransactionDate) == 4) %>% 
  group_by(Day = day(TransactionDate)) %>% 
  dplyr::summarise(trans.recorded = n())

view(april2020)
```

There are no "missing" data apparently, just low values.
Filtering by month number it's possible to check how many stores made a sale during that particular month compared to the total number of stores that made at least one sale during 2020

```{r Number of stores that made a sale}
month2020.wide <- df %>% 
  filter(year(TransactionDate) == 2020) %>% 
  filter(month(TransactionDate) == 4)

length(unique(month2020.wide$StoreKey))
length(unique(df[year(df$TransactionDate) == 2020,]$StoreKey))
```
In March (3) and May (5) 2020, 555 and 550 stores made at least one sale respectively. In April just 513 stores made a sale during that period. Possible Covid-related issues, considering that during the year, the number of active stores (stores that made at least one sale during the year) were 601.

# Days of the week

```{r Checking relative frequencies}
# Re-ordering levels

df$DayOfWeek <- factor(df$DayOfWeek,
                       levels=c('Monday', 'Tuesday', 'Wednesday',
                                'Thursday', 'Friday', 'Saturday',
                                'Sunday'))

# Checking frequencies
table(df$DayOfWeek)

dayoftheweek.plot <- df %>% 
  group_by(year(TransactionDate), df$DayOfWeek) %>% 
  summarise(Frequency = n())


dayoftheweek.plot$Year <- rep(c("2020", "2021", "2022"), 7)

dayoftheweek.plot <- dayoftheweek.plot %>% 
  group_by(Year) %>% 
  mutate(rel.frequency = Frequency/sum(Frequency))

dayoftheweek.plot$Day <- factor(dayoftheweek.plot$`df$DayOfWeek`,
                                levels=c('Monday', 'Tuesday', 'Wednesday',
                                         'Thursday', 'Friday', 'Saturday',
                                         'Sunday'))

ggplot(dayoftheweek.plot, aes(x = Day, y = rel.frequency, fill = Year)) +
  geom_bar(position = "stack", stat = "identity") +
  labs(x = "Category", y = "Relative frequency")
```

Transaction more or less disstributed throughout the week with a peak from Friday to Sunday

## Weekend Flag
```{r WeekendFlag}
table(df$WeekendFlag)
```

# Stores overview

```{r Preparing dataset for the stores}
# Select the year
yr <- "2020"

store.summary <- df %>% 
  select(StoreKey, StoreType, Region_Lvl1, Region_Lvl2, ActualSales, TransactionDate) %>% 
  filter(year(TransactionDate) == yr) %>% 
  group_by(StoreKey, StoreType, Region_Lvl1, Region_Lvl2) %>% 
  summarise(revenues = sum(ActualSales)) %>% 
  mutate(txt = paste("Store: ",StoreKey, "\n",
                     "Yearly sales: ", round(revenues/1000, digits = 0),"kâ‚¬", "\n",
                     "Region lvl.1: ", Region_Lvl1, "\n",
                     "Region lvl.2: ", Region_Lvl2)) %>% 
  arrange(desc(revenues)) %>% 
  ungroup()

# Preparing variables
store.summary$StoreKey <- as.character(store.summary$StoreKey)
```

```{r Packed Bubble Chart}
# Generating Layout
packing <- circleProgressiveLayout(store.summary$revenues, sizetype = "area")
data <- cbind(store.summary, packing)
data.gg <- circleLayoutVertices(packing, npoints = 50)

# Plot
p <- ggplot() +
  geom_polygon_interactive(data = data.gg,
                           aes(x = x, y = y, group = id, fill = id,
                               tooltip = data$txt[id], data_id = id),
                           colour = "black", alpha = 0.6) +
  scale_fill_viridis() +
  geom_text(data = data,
            aes(x = x, y = y, label = StoreKey),
            size = 2, colour = "black") +
  theme_void() + 
  theme(legend.position = "none") +
  coord_equal()

# Interactive
widg <- ggiraph(ggobj = p, width_svg = 7, height_svg = 7)
widg
```

How many stores opened or closed from 2020?
```{r Stores opened/closed}
stores.2020 <- unique(df$StoreKey[df$TransactionDate < as.POSIXct("2021-01-01",format = "%Y-%m-%d", tz = "")])
stores.2022 <- unique(df$StoreKey[df$TransactionDate > as.POSIXct("2022-01-01",format = "%Y-%m-%d", tz = "")])

# Stores closed
stores.2020[!(stores.2020 %in% stores.2022)]
print(paste(length(stores.2020[!(stores.2020 %in% stores.2022)]), "stores closed from 2020",
            sep = " "))
# Stores opened
stores.2022[!(stores.2022 %in% stores.2020)]
print(paste(length(stores.2022[!(stores.2022 %in% stores.2020)]), "stores opened from 2020"))

remove(stores.2020, stores.2022)
```

# ProductCategory
```{r Exploring tables}
table(df$ProductCategory_Lvl2)
```

# Product Category

```{r Preparing data}
# Select product category
prod.cat <- c("AA", "AB", "AC", "AD", "AE")

transactions.sag <- df %>% 
  select(TransactionDate, ActualSales, ProductCategory_Lvl2) %>% 
  filter(ProductCategory_Lvl2 %in% prod.cat) %>% 
  group_by(ProductCategory_Lvl2) %>% 
  arrange(TransactionDate) %>% 
  mutate(cum_rev = cumsum(ActualSales))

```

```{r Plot}
ggplot(data = transactions.sag, aes(x = TransactionDate, y = cum_rev,
                                    fill = ProductCategory_Lvl2)) +
  labs(fill = "Product Category\nLvl. 2", 
       x = "Transaction Date",y = "Cumulative revenues" ) +
  geom_area(color = "black", linewidth = 0.2, alpha = 0.6)
```

AD products are irrelevant when compared to other categories, AE shows a really slow growth, while AA, AB, AC are the best categories in terms of volumes and growth.

```{r}
cat.performances <- transactions.sag %>% 
  group_by(ProductCategory_Lvl2, Year = year(TransactionDate)) %>% 
  summarise(total.sales = sum(ActualSales)) %>% 
  arrange(Year)

sum(cat.performances$total.sales[cat.performances$Year == 2022 & (cat.performances$ProductCategory_Lvl2 == "AA" | cat.performances$ProductCategory_Lvl2 == "AB" | cat.performances$ProductCategory_Lvl2 == "AC")]) / sum(cat.performances$total.sales[cat.performances$Year == 2022])
```


# Distribution channels

```{r Preparing the dataset for the distribution channel}
# Select distribution channel
dist.channel <- c("Physical", "Online")

dist.channel.sag <- df %>% 
  select(TransactionDate, ActualSales, DistributionChannel) %>% 
  filter(DistributionChannel %in% dist.channel) %>% 
  group_by(DistributionChannel) %>% 
  arrange(TransactionDate) %>% 
  mutate(cum_rev = cumsum(ActualSales))

ggplot(data = dist.channel.sag, aes(x = TransactionDate, y = cum_rev,
                                    fill = DistributionChannel)) +
  labs(fill = "Distribution channel", 
       x = "Transaction Date",y = "Cumulative revenues" ) +
  geom_area(color = "black", linewidth = 0.2, alpha = 0.6)
```

Online distribution channel is a very narrow revenues source with low growth

```{r Online Sales Share}
sum(dist.channel.sag$ActualSales[year(dist.channel.sag$TransactionDate) == 2022 & dist.channel.sag$DistributionChannel == "Online"]) / sum(dist.channel.sag$ActualSales[year(dist.channel.sag$TransactionDate) == 2022]) 
```


# Regional distribution

```{r Preparing the dataset}
# Select regions to exclude
regions.exclude.Lvl1 <- c("") # Lvl 1 regions to exclude
regions.exclude.Lvl2 <- c("") # Lvl 2 regions to exclude

reg.distribution.sag <- df %>% 
  select(TransactionDate, ActualSales, Region_Lvl2, Region_Lvl1) %>% 
  filter(!(Region_Lvl1 %in% regions.exclude.Lvl1) | 
           !(Region_Lvl2 %in% regions.exclude.Lvl2)) %>% 
  group_by(Region_Lvl2, Region_Lvl1) %>% 
  arrange(TransactionDate) %>% 
  mutate(cum_rev = cumsum(ActualSales))
```


```{r Empty regions}
empty.regions <- df %>% 
  filter(is.na(Region_Lvl2))
view(empty.regions)
```

StoreKey = 0 is linked to the online distribution channel

# Prices
```{r Negative actual sales}
negative.actual.sales <- df %>% 
  filter(ActualSales <= 0)

summary(negative.actual.sales)
```

```{r Positive discounts}
positive.discounts <- df %>% 
  filter(SalesDiscount > 0)
view(positive.discounts)
```

71 observations with positive discounts, possibly due to errors or price corrections

```{r Unitary price}
################################################
# Select product key to explore
product.code <- c(49340, 49341, 49333, 49329)

# Select starting date
starting.date <- "2020-01-01"
# Select ending date
ending.date <- "2022-12-31"

################################################

# Select the timeframe of analysis
time.interval <- interval(as.POSIXct(starting.date, # Starting date
                                     format = "%Y-%m-%d",
                                     tz = time_at_tz(Sys.timezone(location = TRUE))),
                          as.POSIXct(ending.date, # Ending date
                                     format = "%Y-%m-%d",
                                     tz = time_at_tz(Sys.timezone(location = TRUE))))


price <- df %>% 
  select(TransactionDate, ProductCategory_Lvl2, ProductKey, RetailFullPrice, UnitVolume, CPI_Monthly) %>%
  filter(TransactionDate %within% time.interval) %>% 
  group_by(Year = year(TransactionDate), Month = month(TransactionDate), ProductCategory_Lvl2, ProductKey) %>% 
  summarise(price = sum(RetailFullPrice)/sum(UnitVolume), 
            CPI_Monthly = unique(CPI_Monthly)) %>% 
  ungroup() %>% 
  group_by(Year, Month, ProductCategory_Lvl2) %>% 
  mutate(category.price = mean(price)) %>% 
  arrange(Month) %>% 
  arrange(Year) %>% 
  filter(ProductKey %in% product.code) %>% 
  mutate(Period = paste(Year, Month, sep ="-"))

ggplot(data = price, aes(x = factor(Period, levels = unique(Period)), y = price, group = ProductKey)) +
  geom_line(aes(color = ProductKey)) +
  geom_line(data = price, aes(y = CPI_Monthly), colour = "red") + 
  geom_line(data = price, aes(y = category.price, group = ProductCategory_Lvl2, colour = ProductCategory_Lvl2)) +
  ylab("Price") + 
  xlab("Period") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.8, hjust=0.5))

```


```{r Unitary price distribution}
outliers.actualsales <- df %>% 
  filter(RetailFullPrice > 0 & UnitVolume > 0) %>% 
  mutate(unitary.price = RetailFullPrice/UnitVolume)

summary(outliers.actualsales$unitary.price)
```

```{r}
outliers.transactions <- outliers.actualsales %>% 
  filter(unitary.price >= 41)
```


```{r Plotting price distribution}
retail.price <- outliers.actualsales %>% 
  select(TransactionDate, ProductCategory_Lvl2, unitary.price) %>% 
  group_by(Month = month(TransactionDate), Year = year(TransactionDate), ProductCategory_Lvl2) %>% 
  summarise(avg.price = mean(unitary.price)) %>% 
  mutate(time.interval = as.POSIXct(paste(Year, Month, "01", sep = "-"), 
                                    format = "%Y-%m-%d"))

ggplot(data = retail.price, aes(x = time.interval, y = avg.price, group = ProductCategory_Lvl2,
                                colour = factor(ProductCategory_Lvl2))) +
  geom_line() +
  geom_point() +
  ylab("Average monthly category price") +
  xlab("Date")

```
Huge jump in the average unitary price ticket for categories AD and AE starting from July 2021. A deeper exploration is needed in order to check if the price of the items sold in the categories changed or it is due to an increase in selling of higher priced products belonging to the specific category.

```{r Pareto analysis}
#################################################
# Select the timeframe of analysis
time.interval <- interval(as.POSIXct("2022-01-01",
                                     format = "%Y-%m-%d",
                                     tz = time_at_tz(Sys.timezone(location = TRUE))),
                          as.POSIXct("2022-12-31",
                                     format = "%Y-%m-%d",
                                     tz = time_at_tz(Sys.timezone(location = TRUE))))

# Select the category: AA, AB, AC, AD, AE
category.filter <- c("AC")

# Select the top N products to check
topN <- 20

#################################################

# Creating the pareto dataset for the plot
pareto.analysis <- df %>% 
  filter(ProductCategory_Lvl2 %in% category.filter) %>% 
  filter(TransactionDate %within% time.interval) %>% 
  select(ProductKey, ProductCategory_Lvl2, ActualSales) %>% 
  group_by(ProductKey, ProductCategory_Lvl2) %>% 
  summarise(tot.revenues = sum(ActualSales)) %>% 
  na.omit() %>% 
  ungroup() %>% 
  arrange(desc(tot.revenues)) %>% 
  mutate(per.revenues = base::cumsum(tot.revenues)/sum(tot.revenues)) 


#Creating the dataset with the top N products
topN.products <- pareto.analysis %>% 
  filter(ProductKey %in% pareto.analysis$ProductKey[1:topN])

# Pareto analysis plot
ggplot(data = topN.products, aes(x = factor(ProductKey, levels = ProductKey), y = per.revenues)) +
  geom_bar(stat = "identity", aes(fill = topN.products$tot.revenues, alpha = 0.6)) +
  geom_hline(yintercept = 0.75, colour = "blue") +
  geom_hline(yintercept = 0.90, colour = "red") +
  geom_point() +
  geom_path() +
  scale_x_discrete(breaks = topN.products$ProductKey) +
  ylab("Cumulated revenues") +
  xlab("ProductKey") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        panel.background = element_rect(fill=NA),
        plot.background = element_rect(fill=NA),
        legend.position =  'none',
        axis.title.x = element_blank(),
        axis.title.y = element_blank()) 
# Not visible with many Product Keys
```


# Product deep dive
```{r Deep dive}
################################################
# Select product key to explore
product.code <- 49329

# Select starting date
starting.date <- "2020-01-01"
# Select ending date
ending.date <- "2022-12-31"

################################################

# Select the timeframe of analysis
time.interval <- interval(as.POSIXct(starting.date, # Starting date
                                     format = "%Y-%m-%d",
                                     tz = time_at_tz(Sys.timezone(location = TRUE))),
                          as.POSIXct(ending.date, # Ending date
                                     format = "%Y-%m-%d",
                                     tz = time_at_tz(Sys.timezone(location = TRUE))))

# Preparing the full dataset of transactions
product.deep.dive.df <- df %>% 
  filter(TransactionDate %within% time.interval & 
           ProductKey == as.character(product.code))

# Comparing the sales volumes of the product to those of the category
sales.volumes.product <- product.deep.dive.df %>% 
  group_by(Month = month(TransactionDate), Year = year(TransactionDate)) %>% 
  arrange(Month, Year) %>% 
  summarise(Volumes = sum(UnitVolume),
            ActualSales = sum(ActualSales),
            SalesDiscount = sum(SalesDiscount)) %>% 
  ungroup() %>% 
  mutate(TransactionPeriod = paste(Year, Month, sep = "-")) %>% 
  arrange(Month) %>% 
  arrange(Year) %>% 
  mutate(SalesDeviation = (ActualSales-mean(ActualSales))/sd(ActualSales))

sales.volumes.category <- df %>% 
  filter(TransactionDate %within% time.interval & 
           ProductCategory_Lvl2 == unique(ProductCategory_Lvl2[ProductKey == product.code])) %>% 
  group_by(Month = month(TransactionDate), Year = year(TransactionDate)) %>% 
  arrange(Month, Year) %>% 
  summarise(Volumes = sum(UnitVolume),
            ActualSales = sum(ActualSales),
            SalesDiscount = sum(SalesDiscount)) %>% 
  ungroup() %>% 
  mutate(TransactionPeriod = paste(Year, Month, sep = "-")) %>% 
  arrange(Month) %>% 
  arrange(Year) %>% 
  mutate(SalesDeviation = (ActualSales-mean(ActualSales))/sd(ActualSales))
  
deep.dive.final <- left_join(sales.volumes.product, sales.volumes.category, by = "TransactionPeriod") 


# Plotting sales
ggplot(data = deep.dive.final, aes(x = factor(TransactionPeriod, levels = TransactionPeriod),
                                         y = SalesDeviation.x,
                                         group = 1)) +
  geom_line(aes(y = SalesDeviation.x), colour = "red") +
  geom_line(aes(y = SalesDeviation.y), colour = "blue") +
  geom_point() +
  labs(title = paste("Product code:", as.character(product.code), "Category:",
                      unique(df$ProductCategory_Lvl2[df$ProductKey == product.code]), sep = " "),
       x = "Transaction Period",
       y = "Sales Variation") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=0.5)) 


# Summary statistics: Frequency of distribution channel
print("Transaction frequency:")
table(product.deep.dive.df$DistributionChannel)

```

```{r Brands}
df$BrandKey <- as.factor(df$BrandKey)
print(paste("There are", length(unique(df$BrandKey)), "unique brand keys in the dataset", sep = " "))

brand <- df %>% 
  select(ActualSales, BrandKey) %>%  
  group_by(BrandKey) %>% 
  summarize(BrandImportance = sum(ActualSales))

ggplot(data=brand, aes(x=BrandKey, y=BrandImportance)) +
  geom_bar(stat="identity", fill="steelblue") +
  geom_text(aes(label=round(BrandImportance/sum(BrandImportance),3), vjust=-0.3)) +
  theme_minimal()
  
```
BrandKey 3521 makes almost all the sales (96)


```{r Upstream Bargaining Power?}
df$SupplierKey <- as.factor(df$SupplierKey)
length(unique(df$SupplierKey))

BP <- df %>% 
  group_by(SupplierKey) %>% 
  summarise(Frequency = n(),
            Importance = sum(ActualSales),
            Differentiation = length(unique(ProductKey)))

ggplot(BP, aes(x=Importance, y=Frequency, 
               color=Differentiation, size=Importance)) +
  geom_point() + 
  geom_text(aes(label=SupplierKey, 
            check_overlap = TRUE, 
            vjust = "inward", hjust = "inward")) + 
  theme_minimal() + 
  theme(legend.position = "none")

# Comment: More suppliers are involved in for both a considerable provision in terms of quantity of  goods sold and of value. This should theoretically mean that the company has a good bargaining power. What about those 3 suppliers that provide a number of goods' sold lower than 12? Are their products of low quality? 
```

```{r Sales and Discounts}

SD <- df %>% 
  group_by(week(TransactionDate), month(TransactionDate), year(TransactionDate)) %>% 
  summarise(Daily_Mean = mean(ActualSales),
                Daily_Sum = sum(ActualSales),
                Daily_Discount = -sum(SalesDiscount),
                Volumes = sum(UnitVolume))

cpi <- cpi.df %>% 
  filter(year(Date_daily) > 2019) %>% 
  group_by(M = month(Date_daily), Y = year(Date_daily)) %>% 
  summarise(Month = mean(CPI_Monthly)) %>% 
  arrange(Y,M)

myCPI <- ts(cpi$Month, start=c(2020, 1), end=c(2022, 12), frequency=12)
plot(decoCPI <- stl(myCPI, s.window = "periodic"))


ggplot(comparison, aes(x = T, y = scale(S))) +
  geom_line() +
  geom_line(aes(y = scale(D), colour = "blue")) 

SD$ChangeInDiscount <- c(diff(SD$Daily_Discount),0)
SD$ChangeInSales<- c(diff(SD$Daily_Sum), 0)

colnames(SD)[1] <- "TransactionDate" 


ggplot(SD, aes(x = TransactionDate, y = Daily_Sum)) +
            geom_line(color = "deepskyblue4") + 
            geom_col(aes(x = TransactionDate, 
                         y = Daily_Discount*(min(Daily_Sum)/max(Daily_Discount)))) +
            scale_y_continuous(
            # Features of the first axis
            name = "AggregateSales",
            # Add a second axis and specify its features
            sec.axis = sec_axis(~.*min(SD$Daily_Sum)/max(SD$Daily_Discount), 
            name="AggregateDiscount")) +
            theme(# panel.grid.major = element_blank(),
                  # axis.line = element_line(colour = "black"),
                  axis.title.x = element_blank(),
                  axis.title.y = element_blank(),
                  panel.background = element_rect(fill=NA),
                  plot.background = element_rect(fill=NA))

```


```{r Correlation(Change in Discounts and Sales)}
# Comparison with Changes in Volumes and Discounts
ggplot(SD, aes(x = TransactionDate, y = ChangeInSales)) +
            geom_line(color = "deepskyblue4") + 
            geom_col(aes(x = TransactionDate, 
                         y = ChangeInDiscount)) +
            theme(# panel.grid.major = element_blank(),
                  # axis.line = element_line(colour = "black"),
                  axis.title.x = element_blank(),
                  axis.title.y = element_blank(),
                  panel.background = element_rect(fill=NA),
                  plot.background = element_rect(fill=NA))

cor(SD$ChangeInDiscount, SD$ChangeInSales)

ggplot(SD, aes(x=ChangeInDiscount, y=ChangeInSales)) +
  geom_point() + 
  theme_minimal() + 
  theme(legend.position = "none") + 
  geom_smooth(method=lm,  linetype="dashed",
             color="darkblue", fill="lightblue")

summary(lm1 <- lm(ChangeInSales ~ ChangeInDiscount, data = SD))
plot(rstandard(lm1))

```

```{r Is CPI Relevant for Sales?}
# De-trending Sales

DTS <- transaction.df %>% 
  group_by(M = month(TransactionDate), Y = year(TransactionDate), CPI_Monthly) %>% 
  summarise(Sales = sum(ActualSales),
            Discounts = sum(SalesDiscount)) %>% 
  arrange(M) %>% 
  arrange(Y) %>% 
  mutate(Period = paste(Y, M, sep = "-"))

# The idea: Does CPI explain the Sales' trend? If the coefficient of the regression is statistically significant, yes. Otherwise, no.

plot(actualsales <- ts(DTS$Sales, start = c(2020,1), end = c(2022, 12), frequency = 12))
plot(timeAS <- stl(actualsales, s.window = "periodic"))
trendAS <- timeAS$time.series[,2] # Trend

stl(ts(DTS$Sales, start = c(2020,1), end = c(2022, 12), frequency = 12), s.window = "periodic")$time.series[,2]

Trend <- data.frame(trend = as.numeric(stl(ts(DTS$Sales, 
                                    start = c(2020,1), 
                                    end = c(2022, 12), 
                                    frequency = 12), 
                                 s.window = "periodic")$time.series[,2]),
                    CPI_M = as.numeric(DTS$CPI_Monthly), 
                    Period = as.factor(DTS$Period))


print(myplot <- ggplot(Trend, aes(x = Period, group = 1)) +
            geom_line(aes(y = scale(trend), colour = "Trend")) + 
            geom_line(aes(y = scale(CPI_M), colour = "CPI")) + 
  scale_colour_manual("",
                      breaks = c("Trend", "CPI"),
                      values = c("deepskyblue1", "green4")) +
            theme(# panel.grid.major = element_blank(),
                  # axis.line = element_line(colour = "black"),
                  axis.title.x = element_blank(),
                  axis.title.y = element_blank(),
                  panel.background = element_rect(fill=NA),
                  plot.background = element_rect(fill=NA),
                  axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=0.5)))

geom_line(color = "deepskyblue1")
# It looks like the sales' trend is very similar to CPI's one. Is the latter driving the former? Saying that there is causation is dangerous but if the coefficient in front of CPI is statistically significant, then we might assume that among possible other factors CPI is one of the possible Sales' trend's drivers. 
      
# Is there a trend?
coefficients(summary(lmAS <- lm(trend ~ CPI_M, data = Trend)))[2,4]

# It appears that CPI is extremely significant in explaining the Sales' TREND!

```

```{r Exploring sales and discount for single product}
################################################
# Select product key to explore
product.code <- c(49333, 49341)

# Select starting date
starting.date <- "2021-01-01"
# Select ending date
ending.date <- "2022-12-31"

################################################

# Select the timeframe of analysis
time.interval <- interval(as.POSIXct(starting.date, # Starting date
                                     format = "%Y-%m-%d",
                                     tz = time_at_tz(Sys.timezone(location = TRUE))),
                          as.POSIXct(ending.date, # Ending date
                                     format = "%Y-%m-%d",
                                     tz = time_at_tz(Sys.timezone(location = TRUE))))

# Preparing the full dataset of transactions
product.sales.discounts <- transaction.df %>% 
  filter(TransactionDate %within% time.interval & 
           ProductKey %in% as.character(product.code)) %>% 
  group_by(Year = year(TransactionDate), Week = week(TransactionDate)) %>% 
  summarise(ActualSales = sum(ActualSales), 
            TotalDiscounts = sum(SalesDiscount)) %>% 
  arrange(Week) %>% 
  arrange(Year) %>% 
  mutate(Period = paste(Year, Week, sep = "-")) %>% 
  ungroup()

product.sales <- product.sales.discounts %>% 
  select(Value = ActualSales, Period, Week, Year) %>% 
  mutate(flag = replicate(nrow(product.sales.discounts),"ActualSales"))
  

product.discounts <- product.sales.discounts %>% 
  select(Value = TotalDiscounts, Period, Week, Year) %>% 
  mutate(flag = replicate(nrow(product.sales.discounts),"Discounts"))

product.sales.discounts.final = rbind(product.sales, product.discounts)

product.sales.discounts.final <- product.sales.discounts.final %>% 
  group_by(Year, Week) %>% 
  arrange(Week) %>% 
  arrange(Year)

product.sales.discounts.final$Value[product.sales.discounts.final$flag == "Discounts"] <- -product.sales.discounts.final$Value[product.sales.discounts.final$flag == "Discounts"]

product.sales.discounts.final$Value <- round(product.sales.discounts.final$Value, digits = 0)


ggplot(product.sales.discounts.final, aes(fill=flag,
                                          y=Value,
                                          x=factor(Period, levels = unique(Period)))) + 
  geom_bar(position="stack", stat="identity") + 
  scale_fill_viridis(discrete=TRUE, name="") +
  ylab("Sales Value") + 
  xlab("Year - Week") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=0.5))


```


```{r Holidays vs Normal days}
# What we would like to see is if the promotions and sales have a different behavior (population) in holidays w.r.t. normal days. 

HN <- Sales  %>% 
  group_by(TransactionDate, FestivityFlag) %>% 
  summarise(Sales = sum(ActualSales))

HN$FestivityFlag <- as.factor(HN$FestivityFlag)

S <- ggplot(HN, aes(x=Sales, fill=FestivityFlag)) +
  geom_density(alpha=0.4) + 
  geom_vline(data=plyr::ddply(HN, "FestivityFlag", 
                             summarise, grp.mean=mean(Sales)), 
             aes(xintercept=grp.mean, color=FestivityFlag),
             linetype="dashed")

# Populations are slightly different with different means. Is this difference statistically significant? 

(test<-t.test(Sales ~ FestivityFlag, data = HN))$p.value # The test shows that even at 10% confidence the difference between the two populations would be considered not statistically different from 0. 
test$p.value
# What about Discounts though? 

D <- ggplot(HN, aes(x=Discounts, fill=FestivityFlag)) +
  geom_density(alpha=0.4)

mu <- ddply(HN, "FestivityFlag", summarise, grp.mean=mean(Discounts))
# Add mean lines
D + geom_vline(data=mu, aes(xintercept=grp.mean, color=FestivityFlag),
             linetype="dashed")

# Populations are slightly different with different means. Is this difference statistically significant? 

test <- t.test(Discounts ~ FestivityFlag, data = HN)

# The same conclusion as above can be drawn. 

```

