---
title: "Data Cleaning"
output: html_notebook
editor_options: 
  chunk_output_type: console
---
Setting path
NOTE: In order to run the script smoothly we recommend to have saved the folder
containing all the data sets in a separate folder called "Data-OW"
```{r Path Setting}
path = "../Data-OW/"
```

Importing libraries
```{r Importing Libraries and parameters setting}
pacman::p_load(readxl,lubridate,dplyr,timechange,stringr, 
               ggplot2, ggrepel, tidyverse, ggcharts, tidyquant)

# Removing scientific notation

options(scipen=999)
```

___________________________________________________
# Product dataset

Setting the working directory and importing the datasets
```{r Loading Product.df}
head(product.df <- read.csv(paste0(path,"Hackathon_DimProduct_SAN_vShared.csv"), 
                       header = TRUE), 5)
str(product.df)
```

Cleaning ProductKey Column
Note on RegEx: (i?) is used to render the match case insensitive
```{r Cleaning ProductKey}
length(product.df$ProductKey <- gsub("(i?)key_", "", product.df$ProductKey))

# OK
```

Cleaning Product Sub-Category column. Indeed, lv2. has some categories that needs to be transformed from lower case to upper case before the substitution.
Note on RegEx used: - (\\w*) = takes every word up to infinite times
                    - (\\s)  = considers the first space present in the string
                    - Combination of the two = Take all the words before a space (included)
```{r Cleaning Product Category}
product.df$ProductCategory_Lvl1 <- toupper(gsub("(\\w*\\s)", "", 
                                        product.df$ProductCategory_Lvl1))

product.df$ProductCategory_Lvl2 <- toupper(gsub("(\\w*\\s)", "", 
                                        product.df$ProductCategory_Lvl2))

# toUpper has been used as remedy to the difference in case present
# in the vectors
```

____________________
Quick exploration of categories
```{r Exploration}
table(product.df$ProductCategory_Lvl1) # Only Category A
```

At lv.1 only A category is present in the dataset

```{r Exploration}
table(product.df$ProductCategory_Lvl2)
```
Product suppliers
```{r Exploration}
length(unique(product.df$SupplierKey))
```
There are 17 Product suppliers

```{r}
length(unique(product.df$BrandKey))
```
There are 24 Brands
________________________________________________________________________________
# CPI dataset

```{r Loaging CPI.df} 
head(cpi.df <- read_xlsx(paste0(path, "Consumer Price Index_vShared.xlsx")))
```

Understanding type of variables
```{r Overview}
str(cpi.df)
```

```{r NAs?}
table(is.na(cpi.df)) # Yes, why? This is due to the first column, i.e. the monthyl cpi. 
```

Setting up the date format for CPI
```{r Date format induction}
cpi.df$Date_daily <- as.character(cpi.df$Date_daily)
cpi.df$Date_daily <- as.POSIXct(cpi.df$Date_daily, 
                                format = "%Y-%m-%d",
                                tz = time_at_tz(
                                  Sys.timezone(location = TRUE)))
```

```{r Overview}
summary(cpi.df)
```

```{r Checking for duplicates}
any(duplicated(cpi.df))
```

No duplicates in the dataset

```{r CPI monthly}
# Let's calculate the monthly CPI as the daily average instead of taking the first day-value as benchmark for the entire month

cpi.monthly <- cpi.df %>% 
  group_by(month(Date_daily), year(Date_daily)) %>% 
  summarise(CPI_Monthly = mean(CPI_daily), 
            Date_daily = paste0(year(Date_daily),"-",month(Date_daily), "-", day(Date_daily)))

typeof(cpi.monthly$Date_daily) # Need to be converted
cpi.monthly$Date_daily <- as.POSIXct(cpi.monthly$Date_daily, 
                                format = "%Y-%m-%d",
                                tz = time_at_tz(
                                  Sys.timezone(location = TRUE)))

cpi.df <- left_join(cpi.df, cpi.monthly, by = "Date_daily")
cpi.df <- cpi.df[c( "Date_daily", "CPI_daily", "CPI_Monthly")]
remove(cpi.monthly)

```

```{r Plotting CPI}
ggplot(data = cpi.df, aes(x = Date_daily, y = CPI_daily)) +
  geom_line() +
  geom_vline(xintercept = as.POSIXct("2020-01-01",format = "%Y-%m-%d",
                                     tz = time_at_tz(Sys.timezone(location = TRUE))), 
             colour = "blue") +
  geom_smooth(method=lm, se=FALSE, col='green', size=0.5) +
  labs(x = "Date", y = "CPI", title = "CPI index", subtitle = "2019 - 2022") +
  xlab("Date") +
  ylab("CPI") +
  theme_minimal() +
  theme(text = element_text(size = 20))
  
# Starting from the blue line we have also have data on transactions.
```


________________________________________________________________________________
# Promotion dataset

```{r Laoding Promotion.df}
head(promotion.df <- read.table(paste0(path,"Hackathon_DimPromotion_SAN_vShared.csv"), 
                                header = TRUE,
                                sep = ","))
str(promotion.df)
summary(promotion.df)
View(promotion.df)
```

Transforming Promotion Dates into POSIXct formats
```{r Dates as.Date}
promotion.df$PromotionStartDate <- as.POSIXct(promotion.df$PromotionStartDate,
                                              format = "%m/%d/%Y",
                                              tz = time_at_tz(
                                                Sys.timezone(location = TRUE)))

promotion.df$PromotionEndDate <- as.POSIXct(promotion.df$PromotionEndDate,
                                            format = "%m/%d/%Y",
                                            tz = time_at_tz(
                                              Sys.timezone(location = TRUE)))
```

```{r NAs ?}
table(is.na(promotion.df)) # OK
```


```{r Checking Promotion duration}
promotion.df <- promotion.df %>% 
  mutate(Duration = round(difftime(PromotionEndDate,
                                   PromotionStartDate,
                                   units = "days"),
                          digits = 0))
tail(promotion.df[order(promotion.df$Duration),], 10)

```
These are the promtions we would like to look for. Indeed, since being related to quite long periods
we suppose these might be special promotions for employees. Despite this, we need to understand how many transactions have been made that are associated with these typologies of promotions. If few, we might also remove them from the data set. Note that this analysis has been performed at the end of this script since it requires the merge with other datasets. 

374        342235  Buy $X for $Y         2021-08-16       2031-12-31  3789 days
1516       328509 Percentage off         2018-12-05       2030-12-31  4409 days
2602       269776  Buy $X for $Y         2021-08-10       2049-12-31 10370 days
3043       279596  Buy $X for $Y         2021-08-01       2049-12-31 10379 days
2170       298287        Unknown         2020-12-01       2049-12-31 10622 days
2219       249104        Unknown         2020-09-11       2049-12-31 10703 days
519        252190        Unknown         2020-02-04       2049-12-31 10923 days
1832       323159        Unknown         2020-02-04       2049-12-31 10923 days

```{r Average Promo Duration?}
Promo.AVG.Duration <- promotion.df %>% 
  group_by(PromoMechanic) %>% 
  summarise(AVG.Duration = mean(Duration))

ggplot(data=Promo.AVG.Duration, aes(x=PromoMechanic, y=AVG.Duration)) +
  geom_bar(stat="identity", fill="steelblue") +
  geom_text(aes(label=round(AVG.Duration,3)), vjust=-0.3, size=3.5) +
  theme_minimal()

# A part from the "unknowns", the duration of the rest seems pretty aligned.  
```

```{r Unknown promotions}
view(unknown.promotions <- promotion.df[promotion.df$PromoMechanic == "Unknown",])
```

Some unknown promotions are set without ending date, which as a default is equal to 2049-12-31

```{r Duplicates in promotion}
any(duplicated(promotion.df))
```
No duplicates

OK

________________________________________________________________________________
# Store Dataset

```{r Loading Store.df}
head(store.df <- read.table(paste0(path, 
                                   "Hackathon_DimStore_SAN_vShared.csv"), 
                           header = TRUE,
                           sep = ","))
```

```{r Overview}
str(store.df)
View(store.df)
```

```{r NAs?}
table(is.na(store.df)) # No NAs
```

Note on RegEx: (\S+\s\S+) serves two take the first two words in the string, where \\S+ is the negation of \s, thus taking all the words excluding the space and \\s considers exactly the spaces. The combination takes thus exactly the first two words before a second space 

```{r Cleaning miswritten StoreType}
store.df$StoreType <- gsub("(\\S+\\s\\S+)", "", store.df$StoreType)
```

Adjusting Region_lvl 1 and 2
Note on RegEx: - [A-Z](?!.*[A-Z].* takes just the last capital letter
               - \\d+ is takes just the first group of number
```{r Cleaning miswritten Region_Lvl}
store.df$Region_Lvl1 <- str_extract(store.df$Region_Lvl1, "[A-Z](?!.*[A-Z].*)")
store.df$Region_Lvl2 <- str_extract(store.df$Region_Lvl2, "\\d+")
# Note: The RegEx creates an NA which is due to the fact that line 75 has as Region_lvl 2 "unknown". We will still keep it since it might be relevant for transaction purposes. having said this, we will not be able to allocate it any region in particular. We will leave the "Unknown" 

store.df$Region_Lvl2[which(is.na(store.df$Region_Lvl2))] <- "Unknown"
```

StoreKey = 0 will be checked: If number of transactions is high, we will keep it. Otherwise, it will be removed for sake of clarity. 

Final check
```{r Exploration}
table(store.df$Region_Lvl1)
as.data.frame(sort(table(store.df$Region_Lvl2), decreasing = T))
store.df[which(store.df$StoreType == "Other"),]
```

Interesting observation: Apart from one case, all the "Other" stores types are stemming from
Region A, 30. The company might be interested in checking the reasons why the type of the stores is not well specified. 


```{r Check store key duplicates}
# Check for duplicates
any(duplicated(store.df))
store.df <- distinct(store.df)
```

OK
_____________________________________
# Trasaction Dataset

```{r Loading Transaction.df}
head(transaction.df <- read.table(paste0(path,
                       "Hackathon_FactSalesTransactionDATES_vShared.csv"),
                                  header = TRUE,
                                  sep = ","))
```

```{r Overview}
str(transaction.df)
```

Trasforming TransactionDate into POSIXct format (it takes a while). We directly substitute 2012 with 2021 since we have noticed this possible typo. 
```{r Dates as.Date}
transaction.df$TransactionDate <- gsub("2012", "2021", transaction.df$TransactionDate)
summary(transaction.df$TransactionDate <- as.POSIXct(transaction.df$TransactionDate,
                                                  format = "%Y-%m-%d",
                                                  tz = time_at_tz(
                                                    Sys.timezone(location = TRUE))))

```

During the analysis we noticed a mismatch between transaction.df, store.df and product.df due to a mismatch in the expression of the keys, so we make a few adjustments before proceeding. 

Note on RegEx: \\s* serves to match one or more blank spaces before and after the spcified string
                _?  serves to look for possible underscore before the string and after the space
                [kK][eE][yY] is the word to look for in every possible combination of upper and lower                 case
```{r StoreKey and ProductKey adjustment}
head(transaction.df$StoreKey <- gsub("\\s*_?[kK][eE][yY]\\s*", "", transaction.df$StoreKey), 10)
head(transaction.df$ProductKey <- gsub("\\s*_?[kK][eE][yY]\\s*", "", transaction.df$ProductKey), 10)
```

```{r Checking if there is a match between keys}
length(unique(store.df$StoreKey))
length(unique(transaction.df$StoreKey))

length(unique(product.df$ProductKey))
length(unique(transaction.df$ProductKey))
```

Not all StoreKey in the transactions dataset are present (724 against 725).
There is one ProductKey without a match (299 against 298)

```{r Check ProductKey}
summary((missing.productkey <- transaction.df[!(transaction.df$ProductKey
                                       %in% product.df$ProductKey),]))
```

```{r Check StoreKey}
summary((missing.storekey <- transaction.df[!(transaction.df$StoreKey
                                       %in% store.df$StoreKey),]))
```

```{r NAs?}
summary(transaction.df) # 23978 NA's
```

There are 23978 registered transactions without information that need to be removed

```{r Removing missing values}
transaction.df <- transaction.df[transaction.df$ProductKey != "",]
remove(missing.productkey)
```

```{r Overview}
summary(transaction.df)
```

There are still 72125 missing values in ActualSales
```{r Testing 72125 missing values}
test1 <- transaction.df %>% 
  filter(is.na(transaction.df$ActualSales))

summary(test1)
```

By comparing the two summaries we can see that the full dataset entails SalesDiscount both negative and positive while the subset of transactions where the "ActualSales" are missing has only negative discounts.

-> We can derive: ActualSales = RetailFullPrice + (-SalesDiscount)
since SalesDiscount for missing ActualSales has only negative values

We will later explore positive values of sales discount

Deriving missing ActualSales
```{r NAs imputation}
head(transaction.df$ActualSales[is.na(transaction.df$ActualSales)] <- transaction.df$RetailFullPrice[is.na(transaction.df$ActualSales)] 
+ transaction.df$SalesDiscount[is.na(transaction.df$ActualSales)])
```

Checking again the summary
```{r Overivew}
summary(transaction.df)
```
No more missing values 

```{r Removing Test}
remove(test1)
```

```{r Outliers: UnitVolume}
quantile(transaction.df$UnitVolume, probs = seq(0,1, 0.001))  
head(transaction.df %>% arrange(desc(UnitVolume)), 20)

# Since our final goal is to be able to predict the baseline, we believe that some specific behaviors, such as purchasing 4000 goods in one day, spending 58003.00  with 50% discount is not something usual. Likewise, we can say something similar with all the transactions that have either negative UnitVolumes or 0 (ca. 59.000). For the purpose of our analysis, thus, we will remove these data in order to have a data set that is REPRESENTATIVE OF REALITY. 

transaction.df <- transaction.df[transaction.df$UnitVolume > 0 & transaction.df$UnitVolume <= 33,]
summary(transaction.df)
```

```{r Outliers: Actual Sales}
tail(quantile(transaction.df$ActualSales, probs = seq(0,1, 0.000001)),20) # 99999? Impossible.
# Since we strongly believe that the jump between the 99.7% and 99.8% is too large (from 895 to 99999) we believe these data have been put manually. These will be removed. Moreover, following the same reasoning applied for UnitVolume, we have decided to remove negative values as well. Indeed, despite these could be rare cases of persons that have accumulated fidelty points, representing 0.1% of the populatin, we have opted to remove them. 

transaction.df <- transaction.df[transaction.df$ActualSales > 0 
                                 & transaction.df$ActualSales < 99999 ,]
summary(transaction.df)
```

```{r Outliers: Discount}
tail(quantile(transaction.df$SalesDiscount, probs = seq(0,1, 0.0001)),20)
head(quantile(transaction.df$SalesDiscount, probs = seq(0,1, 0.0001)),20)
table(transaction.df$SalesDiscount > 0) # 65 values larger than 0...What does it mean that the discount is positive? Since I bought something I will pay more? Difficult to believe. Moreover, since representing just 65 out 11 billion cases, we opt to remove them. Furthermore, we decided to remove the first 0.01% since far away from the rest of the "normal" discounts 

transaction.df <- transaction.df[transaction.df$SalesDiscount <= 0 & transaction.df$SalesDiscount > -500,]
summary(transaction.df)
```

OK
________________________________________________________________________________
# Trasaction Promotion Data set
```{r Laoding Transaction.Promotion.df}
head(transaction.promotion.df <-read.table(paste0(path,
                                             "Hackathon_FactSalesTransactionPromotion_vShared.csv"),
                                       header = TRUE,
                                       sep = ","))
```

Transforming TransactionDate into POSIXct format
```{r Dates as.Date}
transaction.promotion.df$TransactionDate <-
  as.POSIXct(transaction.promotion.df$TransactionDate,
                                                       format = "%Y-%m-%d",
                                                       tz = time_at_tz(
                                                         Sys.timezone(location =
                                                                        TRUE)))

```

Explanation on RegEx: See above in section "StoreKey and ProductKey adjustment for transaction.df"
```{r Cleaning StoreKey}
#Checking keys
unique(transaction.promotion.df$StoreKey)
transaction.promotion.df$StoreKey <- gsub("\\s*_?[kK][eE][yY]\\s*", "", transaction.promotion.df$StoreKey)
```

```{r Overview}
summary(transaction.promotion.df)
```

_____________________________________
# Holiday

```{r Laoding Holiday.df}
head(holiday.df <- read.table(paste0(path,"Hackathon_HolidaysMY_vShared.csv"), 
                                       header = TRUE,
                                       sep = ","))
# Holidays
# Data Frame Creation on the basis of holidays.df

holidays <- stack(as.data.frame(cbind(rep("Chinese New Years Day",4),
                                rep("Federal Territory Day",4),
                                rep("Hari Raya Puasa (End of Ramadan)",4),
                                rep("Labor Day",4),
                                rep("Wesak Day (Buddhas Birthday), Kings Birthday",4),
                                rep("Hari Raya Qurban (Feast of Sacrifice)",4),
                                rep("Awal Muharram (Islamic New Year)",4),
                                rep("Merdeka Day (National Day)",4),
                                rep("Milad un Nabi (Birth of the Prophet Muhammad)",4),
                                rep("Deepavali (Festival of Lights)",4),
                                rep("Christmas Day",4))))

Date_holidays <- stack(as.data.frame(cbind(c("2020-01-22", "2021-01-22", "2022-01-22", "2023-01-22"),
                                  c("2020-02-01", "2021-02-01", "2022-02-01", "2023-02-01"),
                                  c("2020-05-24", "2021-05-13", "2022-05-02", "2023-04-22"),
                                  c("2020-05-01", "2021-05-01", "2022-05-01", "2023-05-01"),
                                  c("2020-05-26", "2021-05-26", "2022-05-26", "2023-05-26"),
                                  c("2020-06-31", "2021-07-20", "2022-07-10", "2023-06-29"),
                                  c("2020-08-20", "2021-08-09", "2022-06-30", "2023-07-19"),
                                  c("2020-08-31", "2021-08-31", "2022-08-31", "2023-08-31"),
                                  c("2020-10-28", "2021-10-18", "2022-10-07", "2023-09-26"),
                                  c("2020-11-14", "2021-11-04", "2022-10-24", "2023-10-12"),
                                  c("2020-12-25", "2021-12-25", "2022-12-25", "2023-12-25"))))

holidays.df <- cbind(holidays, Date_holidays)[,-c(2,4)]
colnames(holidays.df) <- c("Festivity", "TransactionDate")

holidays.df$TransactionDate<- as.POSIXct(holidays.df$TransactionDate, format = "%Y-%m-%d",  
                              tz = time_at_tz(Sys.timezone(location = TRUE)))

save(holidays.df, file = paste0(path,"holidays.df.RData"))
```

# Merging data sets

```{r Date as.Date}
cpi.df<- cpi.df[between(cpi.df$Date_daily, as.POSIXct("2020-01-01"), as.POSIXct("2022-12-31")),]

promotion.df <- promotion.df[between(promotion.df$PromotionStartDate, as.POSIXct("2020-01-01"), as.POSIXct("2022-12-31")),]

```

# Creating Dataset for next tasks and the Interactive Dashboard
```{r Merged Data set}
# Trial 
store.df$StoreKey <- as.factor(store.df$StoreKey) # For merge
transaction.df$StoreKey <- as.factor(transaction.df$StoreKey) # For merge

transaction.promotion.df$ProductKey <- as.factor(transaction.promotion.df$ProductKey) # For merge
transaction.df$ProductKey <- as.factor(transaction.df$ProductKey) # For merge

a <- left_join(transaction.df, store.df, by = "StoreKey", multiple = "all")
b <- left_join(a, product.df, by = "ProductKey")
c <- left_join(b, transaction.promotion.df, by =  c("TransactionDate", "StoreKey", "ProductKey"))
d <- left_join(c, holidays.df, by = "TransactionDate")

colnames(cpi.df)[1] <- "TransactionDate" # For merge
e <- left_join(d, cpi.df, by = "TransactionDate")
transaction.df <- left_join(e, promotion.df, by = "PromotionKey") 

save(transaction.df, file = paste0(path,"MergedData.RData"))
```
## Exploration of missing values

```{r Summary}
summary(transaction.df)
```

```{r Checking the strange Promotion Behaviours}
check <- transaction.df %>% 
  filter(PromotionKey %in% c("342235", "328509", "269776", "298287", "249104", "252190", "323159", "279596")) %>% 
  group_by(PromotionKey, PromoMechanic) %>% 
  summarise(N.Transactions = n(), 
            EndDate = unique(PromotionEndDate))

check2 <- transaction.df %>% 
  filter(PromoMechanic == "Unknown") %>%
  group_by(PromotionKey) %>% 
  summarise(N.Transactions = n())

# It appears that no transaction has been made with the majority of those product keys a part from "342235" and "328509". Since this is the case, after the datasets merge our data will not be "polluted" by such promotions. Thus, nothing shall be done. 
  
```

```{r Interactive Dashboard Dataset}

Sales <- transaction.df %>% 
  select(TransactionDate, ActualSales, DistributionChannel, SalesDiscount, ProductCategory_Lvl2, StoreKey, UnitVolume, ProductKey, PromoMechanic, Region_Lvl1, Region_Lvl2, StoreType, CPI_Monthly, CPI_daily, Festivity, RetailFullPrice) %>% 
  mutate(DeflatedSales = ActualSales/CPI_Monthly) %>% 
  mutate(FestivityFlag = case_when(is.na(Festivity) ~ 0,
                                   TRUE ~ 1))

save(Sales, file = paste0(path,"DBData.RData"))
```
